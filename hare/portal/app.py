"""HARE Evaluation Portal â€” Gradio app for human evaluation.

A blind A/B evaluation interface where participants:
1. See an article and headlines generated by different methods
2. Rate each headline on quality, relevance, and personalization
3. Rank the headlines from best to worst

All responses are saved to a JSONL file for analysis.

Usage:
    # Local development
    python -m hare.portal.app

    # With custom data
    python -m hare.portal.app --data results/lamp4_results.json

    # Deploy to HuggingFace Spaces
    python -m hare.portal.app --share
"""

from __future__ import annotations

import argparse
import json
import random
import re
import time
import uuid
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any


# ---------------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------------

@dataclass
class EvalSample:
    """A single evaluation item shown to a participant."""
    sample_id: str
    article: str
    reference_headline: str
    methods: dict[str, str]  # method_name -> generated headline


@dataclass
class ParticipantResponse:
    """A participant's response to one evaluation sample."""
    session_id: str
    participant_id: str
    sample_id: str
    timestamp: float
    ratings: dict[str, dict[str, int]]  # method_label -> {quality, relevance, personalization}
    ranking: list[str]  # method_labels in order from best to worst
    time_spent_seconds: float
    notes: str = ""


# ---------------------------------------------------------------------------
# Data loading
# ---------------------------------------------------------------------------

def load_eval_samples(
    max_samples: int = 20,
    seed: int = 42,
) -> list[EvalSample]:
    """Load evaluation samples from LaMP-4 dev set.

    Generates headlines from each baseline and packages them for blind eval.
    """
    from hare.evaluation.lamp import load_lamp4
    from hare.evaluation.baselines import (
        RandomProfile, MostRecent, InputCopy,
        TfidfRetrieval, BM25Retrieval,
    )

    data = load_lamp4(split="dev", max_samples=max_samples)

    # Use non-neural baselines for the portal (fast, no GPU needed)
    # Neural baselines can be added when a fine-tuned model is available
    baselines = {
        "Random Profile": RandomProfile(seed=seed),
        "Most Recent": MostRecent(),
        "Input Copy": InputCopy(),
        "TF-IDF Retrieval": TfidfRetrieval(),
        "BM25 Retrieval": BM25Retrieval(),
    }

    samples = []
    for s in data.samples:
        methods = {}
        for name, baseline in baselines.items():
            methods[name] = baseline.predict(s.input_text, s.profile)

        article = re.sub(
            r"^Generate a headline for the following article:\s*",
            "", s.input_text, flags=re.IGNORECASE,
        )

        samples.append(EvalSample(
            sample_id=s.id,
            article=article[:500],
            reference_headline=s.target,
            methods=methods,
        ))

    return samples


def load_precomputed_samples(results_path: Path) -> list[EvalSample]:
    """Load evaluation samples from precomputed results JSON."""
    with open(results_path) as f:
        results = json.load(f)

    samples = []
    for name, result in results.get("results", {}).items():
        for pred in result.get("sample_predictions", []):
            # Check if we already have a sample with this id
            existing = next((s for s in samples if s.sample_id == pred["id"]), None)
            if existing:
                existing.methods[name] = pred["prediction"]
            else:
                samples.append(EvalSample(
                    sample_id=pred["id"],
                    article="(article text not available in precomputed results)",
                    reference_headline=pred["reference"],
                    methods={name: pred["prediction"]},
                ))

    return samples


# ---------------------------------------------------------------------------
# Response storage
# ---------------------------------------------------------------------------

class ResponseStore:
    """Persists participant responses to a JSONL file."""

    def __init__(self, output_path: Path) -> None:
        self.output_path = output_path
        self.output_path.parent.mkdir(parents=True, exist_ok=True)

    def save(self, response: ParticipantResponse) -> None:
        with open(self.output_path, "a") as f:
            f.write(json.dumps(asdict(response)) + "\n")

    def count(self) -> int:
        if not self.output_path.exists():
            return 0
        with open(self.output_path) as f:
            return sum(1 for _ in f)


# ---------------------------------------------------------------------------
# Gradio app
# ---------------------------------------------------------------------------

def create_app(
    samples: list[EvalSample],
    store: ResponseStore,
):
    import gradio as gr
    """Build the Gradio evaluation portal."""

    def get_shuffled_methods(sample: EvalSample) -> list[tuple[str, str, str]]:
        """Return (display_label, real_name, headline) tuples, shuffled."""
        items = list(sample.methods.items())
        random.shuffle(items)
        labeled = []
        for i, (real_name, headline) in enumerate(items):
            label = f"Headline {chr(65 + i)}"  # A, B, C, ...
            labeled.append((label, real_name, headline))
        return labeled

    with gr.Blocks(
        title="HARE Evaluation Portal",
        theme=gr.themes.Soft(),
    ) as app:
        # Session state
        session_id = gr.State(lambda: str(uuid.uuid4()))
        current_idx = gr.State(0)
        sample_start_time = gr.State(time.time)
        shuffled_methods = gr.State([])
        participant_id = gr.State("")

        # --- Welcome screen ---
        with gr.Column(visible=True) as welcome_screen:
            gr.Markdown(
                "# HARE Evaluation Portal\n\n"
                "## Personalized News Headline Generation\n\n"
                "Thank you for participating in this evaluation study.\n\n"
                "You will be shown **news articles** along with several "
                "**computer-generated headlines**. Your task is to:\n\n"
                "1. **Rate** each headline on quality, relevance, and personalization (1-5)\n"
                "2. **Rank** the headlines from best to worst\n\n"
                f"There are **{len(samples)} articles** to evaluate. "
                "This should take approximately 10-15 minutes.\n\n"
                "Your responses are anonymous and will be used for academic research only."
            )
            pid_input = gr.Textbox(
                label="Participant ID (any identifier, e.g. your initials)",
                placeholder="e.g. JD",
            )
            start_btn = gr.Button("Begin Evaluation", variant="primary", size="lg")

        # --- Evaluation screen ---
        with gr.Column(visible=False) as eval_screen:
            progress_md = gr.Markdown("**Article 1 / ?**")

            with gr.Accordion("Article", open=True):
                article_text = gr.Markdown("")

            gr.Markdown("---")
            gr.Markdown("### Generated Headlines")
            gr.Markdown(
                "*Rate each headline from 1 (worst) to 5 (best) on three criteria, "
                "then rank them overall.*"
            )

            # Dynamic headline display + rating components
            # We support up to 6 methods
            headline_components = []
            rating_components = []
            for i in range(6):
                label = f"Headline {chr(65 + i)}"
                with gr.Group(visible=False) as group:
                    headline_md = gr.Markdown(f"**{label}**: ...")
                    with gr.Row():
                        quality = gr.Slider(
                            1, 5, step=1, value=3,
                            label="Quality (grammar, fluency)",
                        )
                        relevance = gr.Slider(
                            1, 5, step=1, value=3,
                            label="Relevance (matches article)",
                        )
                        personalization = gr.Slider(
                            1, 5, step=1, value=3,
                            label="Specificity (informative, not generic)",
                        )
                headline_components.append((group, headline_md))
                rating_components.append((quality, relevance, personalization))

            gr.Markdown("---")
            gr.Markdown("### Overall Ranking")
            ranking_input = gr.Textbox(
                label="Rank headlines from best to worst (e.g., 'A, C, B, D')",
                placeholder="A, B, C, ...",
            )
            notes_input = gr.Textbox(
                label="Optional notes",
                placeholder="Any comments about these headlines...",
            )
            submit_btn = gr.Button("Submit & Next", variant="primary", size="lg")

        # --- Done screen ---
        with gr.Column(visible=False) as done_screen:
            gr.Markdown(
                "# Thank you!\n\n"
                "Your responses have been saved. "
                "Thank you for contributing to this research.\n\n"
            )
            response_count = gr.Markdown("")

        # --- Event handlers ---

        def start_eval(pid, sess_id):
            """Initialize evaluation with first sample."""
            if not pid.strip():
                pid = f"anon_{sess_id[:8]}"

            sample = samples[0]
            methods = get_shuffled_methods(sample)

            # Build outputs for headline visibility and content
            updates = [
                gr.Column(visible=False),  # hide welcome
                gr.Column(visible=True),   # show eval
                gr.Column(visible=False),  # hide done
                f"**Article 1 / {len(samples)}**",
                sample.article,
                0,                          # current_idx
                time.time(),                # start time
                methods,                    # shuffled methods
                pid.strip(),                # participant_id
            ]

            # Update each headline slot
            for i in range(6):
                if i < len(methods):
                    label, _, headline = methods[i]
                    updates.append(gr.Group(visible=True))
                    updates.append(f"**{label}**: {headline}")
                else:
                    updates.append(gr.Group(visible=False))
                    updates.append("")

            # Default ranking suggestion
            letters = [chr(65 + i) for i in range(len(methods))]
            updates.append(", ".join(letters))

            return updates

        def submit_and_next(
            idx, sess_id, pid, methods, start_t,
            *slider_and_ranking,
        ):
            """Save response and advance to next sample."""
            # Parse slider values and ranking
            n_methods = len(methods)
            sliders = slider_and_ranking[:n_methods * 3]
            ranking_text = slider_and_ranking[n_methods * 3]
            notes = slider_and_ranking[n_methods * 3 + 1]

            # Build ratings dict
            ratings = {}
            for i, (label, real_name, _) in enumerate(methods):
                q, r, p = sliders[i*3], sliders[i*3+1], sliders[i*3+2]
                ratings[label] = {
                    "method": real_name,
                    "quality": int(q),
                    "relevance": int(r),
                    "personalization": int(p),
                }

            # Parse ranking
            ranking = [r.strip().upper() for r in ranking_text.split(",") if r.strip()]

            # Save response
            response = ParticipantResponse(
                session_id=sess_id,
                participant_id=pid,
                sample_id=samples[idx].sample_id,
                timestamp=time.time(),
                ratings=ratings,
                ranking=ranking,
                time_spent_seconds=round(time.time() - start_t, 1),
                notes=notes or "",
            )
            store.save(response)

            # Advance to next sample
            next_idx = idx + 1
            if next_idx >= len(samples):
                # Done!
                count = store.count()
                return [
                    gr.Column(visible=False),  # hide welcome
                    gr.Column(visible=False),  # hide eval
                    gr.Column(visible=True),   # show done
                    "",  # progress
                    "",  # article
                    next_idx,
                    time.time(),
                    [],
                    pid,
                    *[gr.Group(visible=False) for _ in range(6)],
                    *["" for _ in range(6)],
                    "",  # ranking
                    f"**Total responses collected: {count}**",
                ]

            # Load next sample
            sample = samples[next_idx]
            new_methods = get_shuffled_methods(sample)

            updates = [
                gr.Column(visible=False),  # welcome
                gr.Column(visible=True),   # eval
                gr.Column(visible=False),  # done
                f"**Article {next_idx + 1} / {len(samples)}**",
                sample.article,
                next_idx,
                time.time(),
                new_methods,
                pid,
            ]

            for i in range(6):
                if i < len(new_methods):
                    label, _, headline = new_methods[i]
                    updates.append(gr.Group(visible=True))
                    updates.append(f"**{label}**: {headline}")
                else:
                    updates.append(gr.Group(visible=False))
                    updates.append("")

            letters = [chr(65 + i) for i in range(len(new_methods))]
            updates.append(", ".join(letters))
            updates.append("")  # response_count (not shown yet)

            return updates

        # Wire up start button
        start_outputs = [
            welcome_screen, eval_screen, done_screen,
            progress_md, article_text,
            current_idx, sample_start_time, shuffled_methods, participant_id,
        ]
        for group, md in headline_components:
            start_outputs.extend([group, md])
        start_outputs.append(ranking_input)

        start_btn.click(
            start_eval,
            inputs=[pid_input, session_id],
            outputs=start_outputs,
        )

        # Wire up submit button
        submit_inputs = [
            current_idx, session_id, participant_id,
            shuffled_methods, sample_start_time,
        ]
        for q, r, p in rating_components:
            submit_inputs.extend([q, r, p])
        submit_inputs.extend([ranking_input, notes_input])

        submit_outputs = [
            welcome_screen, eval_screen, done_screen,
            progress_md, article_text,
            current_idx, sample_start_time, shuffled_methods, participant_id,
        ]
        for group, md in headline_components:
            submit_outputs.extend([group, md])
        submit_outputs.extend([ranking_input, response_count])

        submit_btn.click(
            submit_and_next,
            inputs=submit_inputs,
            outputs=submit_outputs,
        )

    return app


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="HARE Evaluation Portal")
    parser.add_argument(
        "--data", type=Path, default=None,
        help="Path to precomputed results JSON. If not set, generates on-the-fly.",
    )
    parser.add_argument("--max-samples", type=int, default=10)
    parser.add_argument(
        "--output", type=Path, default=Path("results/human_eval_responses.jsonl"),
        help="Output path for participant responses.",
    )
    parser.add_argument("--share", action="store_true", help="Create public Gradio link")
    parser.add_argument("--port", type=int, default=7860)
    args = parser.parse_args()

    # Load samples
    if args.data and args.data.exists():
        print(f"Loading precomputed results from {args.data}...")
        samples = load_precomputed_samples(args.data)
    else:
        print(f"Generating evaluation samples (max {args.max_samples})...")
        samples = load_eval_samples(max_samples=args.max_samples)

    print(f"Loaded {len(samples)} evaluation samples")

    # Create response store
    store = ResponseStore(args.output)
    existing = store.count()
    if existing > 0:
        print(f"Found {existing} existing responses in {args.output}")

    # Build and launch app
    app = create_app(samples, store)
    app.launch(
        server_port=args.port,
        share=args.share,
        show_error=True,
    )


if __name__ == "__main__":
    main()
